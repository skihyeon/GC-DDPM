Improving Handwritten OCR with Training
Samples Generated by Glyph Conditional
Denoising Diffusion Probabilistic Model
Haisong Ding
1
⋆
, Bozhi Luan
2 ⋆⋆, Dongnan Gui
2 ⋆⋆, Kai Chen
1 *, and Qiang
Huo
1
1 Microsoft Research Asia, Beijing, China
dinghs11@mail.ustc.edu.cn
, chenkai.cn@hotmail.com
, qianghuo@microsoft.com
2 University of Science and Technology of China {lbz0075,gdn2001}@mail.ustc.edu.cn
Abstract. Constructing a highly accurate handwritten OCR system requires large amounts of representative training data, which is both timeconsuming and expensive to collect. To mitigate the issue, we propose
a denoising diffusion probabilistic model (DDPM) to generate training
samples. This model conditions on a printed glyph image and creates
mappings between printed characters and handwritten images, thus enabling the generation of photo-realistic handwritten samples with diverse
styles and unseen text contents. However, the text contents in synthetic
images are not always consistent with the glyph conditional images, leading to unreliable labels of synthetic samples. To address this issue, we
further propose a progressive data filtering strategy to add those samples
with a high confidence of correctness to the training set. Experimental
results on IAM benchmark task show that OCR model trained with
augmented DDPM-synthesized training samples can achieve about 45%
relative word error rate reduction compared with the one trained on real
data only.
Keywords: handwritten OCR
· handwritten image generation
· denoising diffusion probabilistic model.
1 Introduction
In recent years, researchers in handwritten optical character recognition (OCR)
area are continuously making progress by leveraging advanced model architectures (e.g., [10,48,33,7,28,9,49]). However, it is still a challenging problem, due to
the cursive nature of handwritten strokes and diverse writing styles. To achieve
excellent recognition accuracy for a handwritten OCR system, large amounts of
labeled handwritten images are required. The handwritten image dataset should ⋆ Corresponding author.
⋆⋆ This work was done when Bozhi Luan and Dongnan Gui worked as interns in MMI
Group, Microsoft Research Asia, Beijing, China.
arXiv:2305.19543v1 [cs.CV] 31 May 2023
2 Ding, et al.
be representative enough to cover diverse writing styles and text contents. Obviously, collecting and labeling such a dataset are both time-consuming and
expensive, and existing training data are limited in terms of style and content
coverage. For example, as observed in [31], in the popular IAM dataset [34], a
limited number of samples are collected for each writer, and some words are only
written by a few writers. To handle this content-style data representation issue,
one potential solution is to train a handwritten image generator to synthesize
training samples for handwritten OCR. For any given text and a writer style,
the generator should be able to synthesize photo-realistic handwritten images
that can match the content of the input text and style of the writer.
In the past several years, generative adversarial network (GAN) [13,36,27]
based handwritten image generation methods have achieved promising results.
Most of GAN-based handwritten image generation approaches adopt a text-toimage framework [1,25,11,5,12,3,52,23,47]. Given an input text and a writer embedding, it is able to generate a photo-realistic handwritten image that matches
the content of the input text and style of the writer. However, using text as
input is not sufficiently flexible to embed various contents such as adjacent character interval and character arrangements [31]. By rendering text to a printed
glyph image, SLOGAN [31] proposed to use an image-to-image framework for
handwritten image generation. It is able to generate flexible contents by rearranging characters on the input glyph image. It is noted that these GAN-based
approaches all rely on guidance from an external handwritten recognizer trained
on real data, which implies that the ability of GANs is limited to directly learn
the mapping from texts or printed glyph images to handwritten images without
external recognition model guidance.
Recently, denoising diffusion probabilistic models (DDPMs) [18,44] achieve
superior performances compared with other generation techniques on image generation tasks, including text-to-image generation [6,45,42,43] and image-to-image
generation [46,29]. For handwritten image generation task, [30] investigated a
text-to-image DDPM for online handwritten generation and achieved promising
results. [16] proposed a writer dependent glyph conditional DDPM (GC-DDPM)
for offline handwritten Chinese character generation. GC-DDPM conditions on
a printed glyph image and creates mappings between printed Chinese characters
and handwritten images. Training from samples of a small Chinese character set,
the GC-DDPM is capable of generating photo-realistic handwritten samples of
unseen Chinese character categories. In [16], the DDPM is trained on a largescale handwritten Chinese character database, where the number of training
samples for each writer is relatively sufficient. In this paper, we investigate GCDDPM on the offline English handwritten image generation task. We conduct
experiments on the popular IAM dataset [34] with limited training samples and
content-style representation coverage. We find that even with limited training
data, the GC-DDPM is still able to generate photo-realistic handwritten images.
Since no explicit recognition model guidance is adopted in GC-DDPM, during
sampling, the model can generate noisy samples where the synthesized images do
not match the text contents in glyph conditional images. Directly adding these
Improved Handwritten OCR with GC-DDPM Generated Training Samples 3
samples to the training set for OCR can degrade the recognition performance. To
address this problem, inspired by the self-training framework in automatic speech
recognition [21,39] and OCR [51,50], we propose a progressive data filtering
strategy to add samples with a high confidence of correctness to the training
set. Experimental results on IAM benchmark task show that the performance
of the OCR model can be significantly improved when trained with augmented
DDPM-synthesized samples.
The remainder of this paper is organized as follows. In Section 2, we briefly
review related works. In Section 3, we adopt the GC-DDPM approach in [16] for
offline English handwritten image generation task and introduce the progressive
data filtering strategy. Experimental results are presented in Section 4. Finally,
we conclude this study in Section 5.
2 Related Works
2.1 GAN-based handwritten image generation approaches
Handwritten image generation aims to synthesize offline handwritten images
given input texts. Most of the previous approaches directly use a text-to-image
framework based on GANs. For example, [1] proposed a GAN-based handwritten word image generator with additional guidance from an external handwritten
recognizer trained on real data. [25] further proposed a handwritten generation
model to synthesize handwritten word images that can match the given conditional writing styles. ScrabbleGAN [11] and HiGAN [12] used fully-convolutional
generators, which can generate images of words and text lines with arbitrary
lengths by making the image width proportional to the length of input text. In
[5], the image widths are also automatically learned based on the input text and
style. The text-to-image handwritten image generation framework is further improved with advanced generation model architectures such as self-attention and
deformable convolution layers [3,23,47].
Since only text inputs are leveraged, the generation model needs to learn the
mapping from text embedding to handwritten strokes, which is quite difficult.
Besides pure text, JokerGAN [52] proposed to leverage an additional text line
clue about the existence of “below the baseline” and “above the mean line”
characters to improve the generation model. By rendering text to a printed glyph
image using a standard typeset font, the resulting glyph image obviously contains
more information than text. Using the glyph image as input, SLOGAN [31]
proposed to use an image-to-image framework for handwritten image generation.
It is able to generate flexible contents by changing the positions of characters and
adjusting space interval between adjacent characters in glyph images. Besides
using text or glyph image as input, [15] proposed to synthesize handwritten
images from online handwritten samples based on StyleGAN [27].
Many of the above-mentioned approaches leveraged synthesized handwritten
images as training data to boost the performances of handwritten OCR systems,
and achieved substantial improvements (e.g., [15,23,31]). In this paper, we in-
4 Ding, et al.
vestigate DDPM to synthesize handwritten images to augment the training data
for handwritten OCR.
2.2 Diffusion model
DDPMs [44,18] have been extremely popular in image generation tasks. DDPM
defines a Markov chain of T diffusion steps. In a forward diffusion process, it
slowly corrupts data by adding random noises, then a reverse diffusion process
is learned to recreate data from Gaussian noise. It is shown in [6] that DDPMs
can outperform GANs on image synthesis. In [38], DDPMs for text-to-image
synthesis are explored. The model is able to generate photo-realistic images that
match the content of conditional text with the help of a classifier-free guidance
[19]. Furthermore, in [42,43], DDPMs have demonstrated powerful capabilities
to generate high-quality images given input texts. DDPMs are also successfully
applied to other tasks such as image-to-image generation (e.g., [46]). [30] investigated DDPM for online handwritten generation and achieved promising
results. [16] proposed a GC-DDPM for offline handwritten Chinese character
generation. The GC-DDPM conditions on a printed glyph image and learns the
mappings between printed Chinese character images and handwritten ones. It
is able to generate photo-realistic handwritten images of unseen Chinese character categories. In this paper, we adopt GC-DDPM to generate offline English
handwritten images.
3 Our Approach
3.1 GC-DDPM for handwritten image generation
Given an input text and a writer ID (denoted as w), we adopt a writer dependent
GC-DDPM [16] to generate photo-realistic handwritten images that match the
content of the text and style of the writer. For each input text, we directly render
it to a printed glyph image using a standard glyph font. It is more suitable to
use a glyph image as input because it contains much more information about the
shapes of individual characters than pure text. We denote the glyph image as g.
As shown in Fig. 1a, let x0 denote a data sampled from a real distribution, i.e.,
x0 ∼ q(x) with a corresponding writer ID w and glyph image g. In the forward
diffusion process, small amounts of Gaussian noise are added to x in T steps,
producing a sequence {xt}
T
t=1 calculated as follows:
q(xt|xt−1) = N (xt;
p
1 − βtxt−1, βtI), xt =
√
αtxt−1 +
√
1 − αtϵt , (1)
where ϵt ∼ N (0, I), βt ∈ (0, 1) and αt = 1 − βt. It is easy to calculate that
q(xt|x0) = N (xt;
√
α¯tx0,(1 − α¯t)I), xt =
√
α¯tx0 +
√
1 − α¯tϵ , (2)
where ϵ ∼ N (0, I), ¯αt =
Qt
i=1 αi
. When T → ∞, ¯αT → 0, and xT ∈ N (0, I).
A nice property of the forward diffusion process is that the reverse conditional
probability is Gaussian when conditioned on x0:
q(xt−1|xt, x0) = N (xt−1; µ˜(xt, x0), β˜
tI) , (3)
Improved Handwritten OCR with GC-DDPM Generated Training Samples 5
(a) The Markov chain of forward diffusion process q(xt|xt−1) and the learned
reverse diffusion process pθ(xt−1|xt) of DDPM.
Writer
Embedding
Timestep
Embedding
Writer ID Normalize FFN t
Conv Conv
Font-Rendered Glyph Image
Noised handwritten image of step t
U-Net
Gaussian noise distribution of t-th step(b) The architecture of the U-Net to estimate ϵθ(xt, g, w) and Σθ(xt, g, w).
Figures are adapted from [16].
Fig. 1: Illustration of GC-DDPM framework for handwritten image generation.
where
µ˜(xt, x0) = 1
√
αt
(xt −
1 − αt √
1 − α¯t
ϵt), β˜
t =
1 − α¯t−1
1 − α¯t
· βt . (4)
Moreover, the reverse process as shown in Fig. 1a will also be a Gaussian when
βt is sufficiently small. Therefore, we can learn a model pθ to approximate the
reverse process conditioned on g and w:
pθ(xt−1|xt, g, w) = N (xt−1; µθ(xt, g, w), Σθ(xt, g, w)) . (5)
Following [37,16], µθ(xt, g, w) and Σθ(xt, g, w) are re-parameterized as
µθ(xt, g, w) = 1
√
αt

xt −
1 − αt √
1 − α¯t
ϵθ(xt, g, w)

(6)
Σθ(xt, g, w) = exp 
νθ(xt, g, w) log βt + (1 − νθ(xt, g, w)) log β˜
t

.
A neural network is trained to estimate ϵθ(xt, g, w) and Σθ(xt, g, w). We
use the same hybrid objective function as in [37]. After the reverse process is
learned, conditioned on g and w, we are able to draw samples x0 according to
Eqn. (5), starting with a Gaussian noise xT ∼ N (0, I).
We adopt the same U-Net architecture in GC-DDPM [6,16] in our task. As
shown in Fig. 1b, xt and g are normalized to a fixed size. Then they are concatenated together and used as input to the U-Net. Time step t is embedded with
sinusoidal embedding, and then processed with a 2-layer feed-forward network
(FFN). Writer information w is embedded with a learnable embedding, followed
by L2-normalization: z = w/∥w∥2. Finally, they are added together and fed to
layers in U-Net using a feature-wise linear modulation (FiLM) operator [40].
6 Ding, et al.
In DDPM, classifier-free guidance [19] is an effective approach to improve
generation quality. Following [16], a content guidance scale γ and a style content
scale η are used. During sampling, ϵθ(xt, g, w) is directly replaced with
ϵ˜θ(xt, g, w) = ϵθ(xt, g, w) + γϵθ(xt, g, ∅) (7)
+ ηϵθ(xt, ∅, w) − (γ + η)ϵθ(xt∅, ∅) .
Here ϵθ(xt, g, ∅), ϵθ(xt, ∅, w) and ϵθ(xt, ∅, ∅) are trained together with ϵθ(xt, g, w)
using the same U-Net, where w, g or both are replaced with a special token ∅.
Works in [16] synthesize offline Chinese character images which are of fixed
height and width. While in English handwritten image generation task, the
widths of the generated images should be inferred from the text and writer
style. To achieve this goal, we prepare handwritten images of words and short
phrases to train GC-DDPM, where the maximum aspect ratio of images is set
as 8. First, images are resized to a height of 64 while keeping aspect ratio. Then,
images are padded to a width of 512 with black pixels on both left and right
margins. The glyph images are also processed to the size of 64 × 512 using the
same procedure. In experiments, we find that during sampling, GC-DDPM will
learn the width of black margins based on input text and writer style. It will
generate handwritten images with clear black margins robustly. Therefore, to
get the final handwritten sample, we use a simple image processing method to
remove the padded black margins.
3.2 Progressive data filtering strategy
By conditioning on glyph images and writer IDs, we expect GC-DDPM to learn
the mapping from glyph images to handwritten ones and generate high-quality
training data to improve handwritten OCR systems. However, we notice that the
generated images are not always consistent with the glyph conditional images.
Adding these data to the training set would degrade the performance and robustness of the handwritten OCR system. To alleviate this problem, we propose
a progressive data filtering strategy to remove these noisy samples.
In text-to-image generation tasks, a dot product score between text and
image embeddings is used as a metric to select generated samples to improve
generation quality [42]. In the self-training framework, for each unlabeled sample
with a pseudo label, a confidence score is calculated. Only data with high confidence scores are added to the training set [21,39,51,50]. Inspired by these works,
we design a metric to estimate the “confidence of correctness” for each generated handwritten image. Then samples with a high confidence of correctness are
added to the training set progressively.
Let R = {(xi
, yi
)} denote a real dataset with handwritten image xi and
ground truth label yi
. Let S = {(x˜j , y˜j
)} denote DDPM-generated synthetic
dataset with handwritten image x˜j and corresponding conditional text y˜j
. First,
we train an initial OCR model M using R only. Then the trained model is used
Improved Handwritten OCR with GC-DDPM Generated Training Samples 7
Algorithm 1: Progressive data filtering strategy
Input: Real data R = {(xi, yi
)}, synthetic data S = {(x˜j , y˜j
)}, initial
selected synthetic data S
′ = {}, number of progressive data filtering
rounds N, data filtering threshold τ
1 Train model M using R;
2 for n ← 1 to N do
3 S
′ =

(x˜j , y˜j
) ∈ S | c(x˜j , y˜j
;M) ≥ τ
	
;
4 Train model M using R ∪ S
′
starting from random weight initialization;
5 return M, S
′
;
to calculate the confidence of correctness score for each x˜j in S as follows:
c(x˜j , y˜j
;M) =
L(x˜j , yˆj
;M)
L(x˜j , y˜j
;M)
, (8)
where L(x, y;M) = − log p(y|x;M) is the negative log posterior probability
calculated using recognizer M, and yˆj
is the decoding result of x˜j using M.
Obviously, if y˜j = yˆj
, the score equals to 1, meaning that the recognizer’s
prediction is consistent with the conditional text. Then the confidence of x˜
matching y˜ is high. If L(x˜j , yˆj
;M) ≪ L(x˜j , y˜j
;M), the score is close to 0,
and the confidence of x˜ matching y˜ is low. In practice, a threshold τ ∈ (0, 1] is
used, and (x˜j , y˜j
) with c(x˜j , y˜j
;M) ≥ τ is included in a selected set S
′
. Then,
a new OCR model can be trained with R ∪ S
′
. After that, the scores can be
re-calculated using the new model. This process can be repeated for multiple
rounds until the performance of the OCR model does not improve further. The
whole progressive data filtering strategy is summarized in Algorithm 1.
4 Experiments
4.1 Experimental setup
We conduct experiments on the IAM dataset [34]. It contains 13,353 isolated text
line images and 115,320 word images written by 657 different writers. We use
the RWTH Aachen partition as in [22] in experiments. Following [6], diffusion
step number T is set as 1,000 with a linear noise schedule. During training, w
and g are randomly set to ∅ with probability 10%, independently. When g = ∅,
a blank glyph image will be used; when w = ∅, a special embedding will be
used. During sampling, we use DDIM [20] sampling method with 50 steps to
save sampling time. As for the handwritten OCR system, the same CTC-based
[14] model in [31] is used without leveraging external language models, and yˆj
in Eqn. (8) is decoded with the best path decoding algorithm. The total number
of parameters of the OCR model is 14M. We mainly conduct experiments on the
IAM word benchmark, except in Section 4.6 where we conduct experiments on
IAM text line benchmark. A word error rate (WER) of 19.47% and a character
8 Ding, et al.
(b) Real handwritten samples
(c) Synthetic handwritten samples
Style guidance scale 𝜂 Content guidance scale
γ
0.0 1.0 0.0 1.0 2.0
2.0 3.0 4.0
(a) Printed glyph image
Fig. 2: Real samples of words “Anglesey” written by writer 333 and synthetic
handwritten images generated with different guidance scales. Glyph conditional
image is shown in (a).
error rate (CER) of 7.27% is achieved when trained on real IAM word data only,
which is similar to the baseline result (19.12% WER and 7.39% CER) presented
in [31].
4.2 Effect of classifier-free guidance scales in GC-DDPM
Works in [19,16] show that the classifier guidance scale is able to control the
trade-off between the quality and diversity of generated samples. Fig. 2 (b)
shows real samples of “Anglesey” written by writer 333. Clearly, the style and
position of individual characters vary each time the same writer writes them. Fig.
2 (c) visualizes synthetic samples generated with different guidance scales. With
higher content guidance scales, the generated samples become less diverse, which
is consistent with the observation in [16]. For example, in real samples, the “le”
in “Anglesey” is either separately written, or consecutively written with a single
stroke. Synthetic samples with lower content guidance scales successfully capture
both writing variants. Whereas samples with higher content guidance scales
only capture the consecutively written one. We also observe that the variance
in generated image widths becomes smaller when sampled with higher content
guidance scales. As for the style guidance scales, since the writer ID is already
a distinctive guidance, the sampling qualities with different scales are similar.
Improved Handwritten OCR with GC-DDPM Generated Training Samples 9
Table 1: WER and CER of handwritten OCR models on IAM word testing set
trained with synthetic dataset generated with different guidance scales.
Style scale η Content scale γ WER (%) CER (%)
0.0
0.0 20.17 7.50
0.5 21.06 7.94
1.0 21.93 8.23
0.5
0.0 20.35 7.52
0.5 21.14 7.91
1.0 21.67 8.14
1.0
0.0 20.40 7.59
0.5 20.41 7.56
1.0 21.25 7.91
IAM training set 19.47 7.27
Table 2: WER and CER of handwritten OCR models on IAM word testing set
trained with different training sets.
Training set WER(%) CER(%)
IAM training set 19.47 7.27
+ Synth-IAM-Words 11.57 3.88
+ Synth-EN-Words 14.78 5.14
+ Synth-EN-Words-WI 14.83 5.18
To evaluate the behavior of guidance scales in generating training data for
handwritten OCR, we try γ, η ∈ {0.0, 0.5, 1.0} and synthesize the whole IAM
word training set using the exact word corpus and writer IDs. The number of
synthetic images equals to the number of images in the IAM training set. Then,
we use synthetic data only to train an OCR model and evaluate its recognition
performance on the real IAM word testing set. As shown in Table 1, with the
same η, WER increases as γ becomes higher. This shows that the diversity of
generated images is important when synthesizing training data for OCR. The
best recognition performance is achieved with both guidance scales set as 0. The
best WER is only absolute 0.7% worse than that trained on real dataset, which
demonstrates the high quality of DDPM-generated handwritten images. Based
on these observations, we set γ = 0.0, η = 0.0 in the following experiments.
In Fig. 3, we show generated samples conditioned by different writer IDs with
input words that are seen in IAM training set and out-of-vocabulary words. It
is clear that GC-DDPM is able to synthesize these words while mimicing the
writing styles (e.g., cursive, slant, stroke pattern) of the conditional writers.
It is noted that although “Z” only appears four times in the training set, the
GC-DDPM still can generate high quality handwritten images.
10 Ding, et al.
temperatures
violently
comprehensive
famille
Zimbabwe
certes
nominating opposed delegations shopgirls
Labour institution Relations will
Fig. 3: Synthetic handwritten images conditioned by different writer IDs with
different words. Top: real samples with corresponding words from IAM writers
001, 002, 023 and 027, respectively. Middle: synthetic samples of words that are
seen in IAM training set. Bottom: synthetic samples of out-of-vocabulary words.
𝐳ଵ 𝐳 = 𝐳
𝐳ଶ ଵ cos
𝜆𝜋
2 + 𝐳ଶ sin
𝜆𝜋
2
Interpolation factor 𝜆
0 1/4 1/2 3/4 1 Fig. 4: Synthetic handwritten images of word “vector” generated using writer
style interpolations between z1 and z2.
4.3 Augment training set with synthetic images for OCR
Next, we use synthetic handwritten images to boost handwritten OCR performance. To evaluate the quality of generated samples of seen/unseen words, three
sets of synthetic handwritten images are generated in our experiments:
• Synth-IAM-Words: Since the IAM training set is insufficient in terms of content and style coverage, we use GC-DDPM to generate handwritten images
for each writer. For each of the 442 writers observed in the training set, we
synthesize a handwritten image for each and every word in the entire IAM
word training corpus. As a result, the Synth-IAM-Words dataset is 442 times
the size of the original training set in terms of the number of samples.
• Synth-EN-Words: Samples in Synth-IAM-Words only contain words that
have been observed in training set. To investigate the quality of synthesized
handwritten samples of unseen words, following [11,31], an external “English
words” 1
corpus is used. It contains 466,550 unique words, 98.9% of which are
not observed in the IAM training set. To generate a diverse dataset, for each
1
https://github.com/dwyl/english-words/blob/master/words.txt
Improved Handwritten OCR with GC-DDPM Generated Training Samples 11
word, we synthesize 8 samples conditioning on 8 randomly selected writer
IDs. As a result, Synth-EN-Words dataset contains about 3.7M samples.
• Synth-EN-Words-WI: The above two datasets are generated with trained
writer IDs. The GC-DDPM is also able to generate unseen styles using writer
style interpolations [16]. To achieve this, given two normalized writer embedding z1 and z2, a new embedding z can be obtained with spherical interpolation [42]: z = z1 cos λπ
2 + z2 sin λπ
2 with interpolation factor λ ∈ [0, 1]. Fig. 4
shows handwritten samples generated using writer style interpolations. It is
clear that as λ increases from 0 to 1, the style of generated samples gradually
shifts from z1 to z2. To evaluate the quality of synthesized handwritten samples with interpolated styles, for each word in “English words” corpus, we
also synthesize 8 samples conditioning on 8 randomly calculated writer interpolations. We use λ = 1/2. We name this dataset “Synth-EN-Words-WI”.
It also contains 3.7M samples.
Table 2 lists the performances of handwritten OCR models on IAM word
testing set trained with different training sets. It is clear that augmenting the
training set with synthetic handwritten images can significantly boost the recognition performances. Specifically, a 40.6% WER reduction (WERR) and a 46.6%
CER reduction (CERR) are achieved with Synth-IAM-Words dataset. It shows
that the generated Synth-IAM-Words can successfully alleviate the insufficient
content and style coverage problem in training set and achieves significant OCR
performance improvements. Augmenting the training set with Synth-EN-Words
and Synth-EN-Words-WI achieves similar recognition accuracy improvements
(about 24% WERR and 29% CERR), which suggests that the synthesized qualities of Synth-EN-Words and Synth-EN-Words-WI are similar.
In our experiments, we construct synthetic datasets using two corpora, the
IAM corpus and an external “English words” corpus. Words in IAM corpus
are seen in the training of DDPM, whereas most of words in “English words”
corpus are unseen. We find that the quality of synthesized data of words in
“English words” is worse than that of words in IAM corpus. This shows that the
synthesized data quality of unseen words is worse than that of seen words.
To further show the quality difference between Synth-IAM-Words and SynthEN-Words/Synth-EN-Words-WI, we treat their conditional texts as ground truths
and evaluate the WER using the OCR model trained on IAM training set. A 22%
WER is observed on Synth-IAM-Words, while the WERs on Synth-EN-Words
and Synth-EN-Words-WI are 73% and 72%, which are significantly higher. There
are two potential reasons for this observation: (a) the generalization ability of
the OCR model trained with IAM training set is limited when recognizing unseen words, and (b) the quality of DDPM-synthesized handwritten images of
unseen words is worse than seen words. As a comparison, we evaluate the OCR
model on a subset of IAM word testing set containing unseen words and achieve
a 40.8% WER. Therefore, both reasons contribute to the high WER. Based on
these analyses, we conclude that the text contents in synthetic images are not
always consistent with the glyph conditional images, leading to unreliable labels
of synthetic data. Adding noisy data to the training set could degrade the per-
12 Ding, et al.
Table 3: WER and CER of handwritten OCR models on IAM word testing set
trained on augmented synthetic datasets with progressive data filtering strategy.
Synthetic dataset N
τ = 1.0 τ = 0.7 Use all samples
WER(%) CER(%) WER(%) CER(%) WER(%) CER(%)
Synth-IAM-Words
1 12.70 4.60 12.40 4.45
2 11.84 4.23 11.73 4.16 11.57 3.88
3 11.77 4.22 11.54 4.14
Synth-EN-Words
1 15.33 5.38 14.53 5.09
2 14.38 5.01 14.13 4.86 14.78 5.14
3 14.12 4.88 13.85 4.83
Synth-EN-Words-WI
1 15.18 5.30 14.39 4.96
2 14.43 4.99 14.20 4.96 14.83 5.18
3 14.15 4.92 14.06 4.85
Esquimalt
𝑐 = 0.6584
GRAHAM
𝑐 = 0.5378
eye
𝑐 = 0.4682
Pholsena
𝑐 = 0.3780
Adhem
𝑐 = 0.2825
Second
𝑐 = 0.1795
K’s
𝑐 = 0.0842
State
𝑐 = 0.0017
F-scope
𝑐 = 0.6070
Berossos
𝑐 = 0.5015
Diplodia
𝑐 = 0.1795
bizz
𝑐 = 0.0000
charpai
𝑐 = 0.0621
depolymerize
𝑐 = 0.0002
fliting
𝑐 = 0.0110
chylocaulously
𝑐 = 0.0068
Fig. 5: Samples of synthetic handwritten images in (top) Synth-IAM-Words and
(bottom) Synth-EN-Words sets that are removed using progressive data filtering.
Wrongly generated characters are highlighted in red. Confidence of correctness
scores are listed below.
formance of the OCR model. Next, we will leverage the progressive data filtering
strategy to remove synthetic data with unreliable labels.
4.4 Effect of progressive data filtering strategy
To evaluate the effect of the progressive data filtering strategy, we set the number of data filtering rounds N = 3, and try τ = {1.0, 0.7}. We conduct experiments on three synthetic dataset and results are listed in Table 3. We also list
the baseline results when all synthetic samples are added to the training set.
The performances of OCR models improve with more progressive data filtering
rounds. We also try to use an additional 4th round, but no further performance
improvements are observed. Compared with τ = 1.0, better performances are
achieved using τ = 0.7. This implies that τ = 0.7 achieves a better tradeoff
Improved Handwritten OCR with GC-DDPM Generated Training Samples 13
Alonso et al. ScrabbleGAN SLOGAN GC-DDPM (Ours)
Fig. 6: Visual comparisons with Alonso et al. [1], ScrabbleGAN [11] and SLOGAN [31]. The words generated from top to bottoms are: olibrius, inventif, bonjour, ionique, malade, golf, ski. The writer IDs of our generated samples are 135,
111, 011, 023, 001, 002, 027, respectively.
between numbers of high-quality and noisy samples. Compared with using all
generated samples, progressive data filtering achieves slightly better WER and
worse CER on Synth-IAM-Words. After 3 rounds, about 90% and 91% of samples in Synth-IAM-Words are added to the training set with τ = 1.0 and τ = 0.7,
respectively. On Synth-EN-Words and Synth-EN-Words-WI datasets, progressive data filtering achieves much better results than using all generated samples.
With progressive data filtering, about 55% and 59% of the data are added to
the training set with τ = 1.0 and τ = 0.7, respectively. Fig. 5 shows samples of
synthetic handwritten images that are removed using progressive data filtering.
The generated images can contain errors, such as missing or repeating certain
characters, or failing to distinguish some easily confused characters. The proposed data filtering strategy can successfully remove these error samples. These
results show that DDPM-generated samples of unseen words are much noisier
than samples of seen words, and the progressive data filtering strategy is helpful
to remove noisy samples and achieve better OCR performance.
4.5 Comparison with previous methods
Fig. 6 shows visual comparisons with previous methods. Our GC-DDPM approach can generate photo-realistic handwritten images with fewer artifacts. In
ScrabbleGAN [11] and SLOGAN [31], the width of the generated image is determined by the length of input text or the width of glyph conditional image. Our
approach is able to generate images with variable widths according to the text
content and writer style. GC-DDPM can successfully mimic the unique style of
the conditional writer styles. For example, writer 111 usually writes “t” similarly
with “T”, and the generated stroke of “t” in “inventif” is also similar to “T”.
Besides visual comparison, we also compare the synthetic data quality using
FID metric. Following GANwriting [25] and SLOGAN [31], we generate 400
unique out-of-vocabulary (OOV) words and calculate an averaged FID [17] score
14 Ding, et al.
Table 4: Comparison with previous methods on IAM word testing set. No lexicons and language models are applied.
Method Synthetic data WER(%) CER(%)
Kang et al. [22] No 16.39 6.43
Learn to Augment [32] + AFDM [4] No 13.35 5.13
SLOGAN (Baseline) [31] No 19.12 7.39
Dutta et al. [8] Font-based 12.61 4.88
Kang et al. [26] Font-based 17.26 6.75
SLOGAN [31] GAN-based 14.97 5.95
SLOGAN [31] + Learn to Augment [32] GAN-based 12.90 4.94
Ours (Real) No 19.47 7.27
Ours (Real + filtered synthetic data) DDPM-based 10.72 3.75
Ours (Filtered synthetic data only) DDPM-based 11.55 4.07
Table 5: Comparison of FID on out-of-vocabulary word images.
Method GANwriting [25] SLOGAN [31] GC-DDPM (ours)
FID 125.87 97.81 86.93
of each handwriting style 2
. As shown in Table 5, we achieve an FID score of
86.93 which is better than that of both GANwriting and SLOGAN.
Since the goal of our approach is to generate handwritten images to augment
the training set for handwritten OCR, we compare our approach with other
synthetic data augmented OCR systems on the IAM word benchmark. To push
the OCR performance to limit, we add all samples in Synth-IAM-Words, filtered
Synth-EN-Words and Synth-EN-Words using τ = 0.7 to augment the IAM training data. As shown in Table 4, we achieve a 10.72% WER and a 3.75% CER,
which are much better than previous methods using font rendered or GAN-based
synthetic images. Compared with using real IAM training data alone, we achieve
a relative 45% WERR and a relative 48% CERR. We also conduct an experiment
of using only filtered synthetic data, and achieve a 11.55% WER and a 4.07%
CER, which are significantly better than using IAM real training data only.
4.6 Experiments on IAM text line dataset
Finally, we conduct experiments on the IAM text line benchmark. Although the
GC-DDPM is trained on images with maximum aspect ratio of 8, it can generate
text lines as shown in Fig. 7. We use the same CTC-based OCR model as in
IAM word experiments, and achieve a 7.05% CER. For synthetic data, besides
using the filtered synthetic word dataset, we also synthesize handwritten text
line samples using the IAM training line corpus, and filter these samples using
2 According to the authors of GANwriting, the exact list of 400 OOV words is no
longer available. Therefore, we follow their advice to build our own OOV word list.
Improved Handwritten OCR with GC-DDPM Generated Training Samples 15
Fig. 7: Samples of synthetic handwritten text line images of a sentence: “The
quick brown fox jumps over the lazy dog.” .
Table 6: Comparison with previous methods on IAM text line testing set. No
lexicons and language models are applied.
Method Synthetic data WER(%) CER(%)
Puigcerver [41] No 18.40 5.80
Michael et al. [35] No - 5.24
Wick et al. [48] No - 5.67
Dutta et al. [8] Font-based 17.82 5.70
Barrere et al. [2] Font-based 16.31 4.76
Kang et al. [24] Font-based 15.45 4.67
Wick et al. (CTC) [49] Font-based 16.85 4.99
Wick et al. [49] Font-based 12.20 3.96
TrOCRSMALL [28] Font-based - 4.22
Ours (Real) No 22.11 7.05
Ours (Real + filtered synthetic data) DDPM-based 13.08 4.13
progressive data filtering. As shown in Table 6, we finally achieve a 4.13% CER,
which is slightly better than TrOCRSMALL. It should be noted that works in
[49,28] use advanced sequence-to-sequence framework for OCR. [49] achieves a
4.99% CER with CTC-based model. TrOCRSMALL also leverages pre-trained
encoder and decoder with 62M total parameters. We only use a simple CTCbased OCR model with 14M parameters, without using any image pre-processing
technique and pre-trained models.
5 Conclusion
In this paper, we investigate GC-DDPM to generate handwritten images to augment training data for handwritten OCR. The proposed GC-DDPM is able to
generate photo-realistic handwritten samples with diverse styles and text contents. However, we find that the text contents in synthetic images are not always
consistent with the glyph conditional images, especially in images with out-ofvocabulary words. Therefore, we further propose a progressive data filtering
method to remove samples with noisy labels. Experiments on both IAM word
and text line benchmarks show that the performance of the OCR model trained
with augmented DDPM-synthesized samples can perform much better than the
one trained on real data only.
16 Ding, et al.
References
1. Alonso, E., Moysset, B., Messina, R.O